# é™é™678çš„å­¦ä¹ ç¬”è®°
# A
## å®‰è£…
### å®‰è£…pytorchï¼š
    pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html
### å®‰è£…tensorboard
    pip install pytorch-transformers tensorboardX
# B
# C
## conda
### è·å–ç‰ˆæœ¬å·/å¸®åŠ©
    è·å–ç‰ˆæœ¬å·		        conda -V
    è·å–å¸®åŠ©	            conda -h
    è·å–ç¯å¢ƒç›¸å…³å‘½ä»¤çš„å¸®åŠ©	 conda env -h
### ç¯å¢ƒç›¸å…³
    åˆ›å»ºç¯å¢ƒ	conda create -n environment_name
    åˆ›å»ºæŒ‡å®špythonç‰ˆæœ¬ä¸‹åŒ…å«æŸäº›åŒ…çš„ç¯å¢ƒ	conda create -n environment_name python=3.7 numpy scipy
    è¿›å…¥ç¯å¢ƒ	conda activate environment_name
    é€€å‡ºç¯å¢ƒ	conda deactivate
    åˆ é™¤ç¯å¢ƒ	conda remove -n yourname --all
    åˆ—å‡ºç¯å¢ƒ	conda env list / conda info -e
    å¤åˆ¶ç¯å¢ƒ	conda create --name new_env_name --clone old_env_name
    æŒ‡å®šç›®å½•ä¸‹ç”Ÿæˆç¯å¢ƒymlæ–‡ä»¶	 conda env export > ç›®å½•/environment.yml
    ä»ymlæ–‡ä»¶åˆ›å»ºç¯å¢ƒ	        conda env create -n env_name -f environment.yml
### ç®¡ç†åŒ…
    å¯¹åŒ…çš„ç®¡ç†æ˜¯åœ¨æŸä¸ªç¯å¢ƒä¸‹è¿›è¡Œçš„ï¼Œå…ˆè¿›å…¥ç‰¹å®šç¯å¢ƒå†è¿›è¡ŒåŒ…çš„æ“ä½œæ¯”è¾ƒå¥½ï¼Œä¸ä¼šå‡ºç°æŠŠæœ¬è¯¥å®‰è£…åœ¨Aç¯å¢ƒä¸­çš„åŒ…å®‰è£…åœ¨äº†Bç¯å¢ƒä¸­è¿™ç§æƒ…å†µã€‚
    å®‰è£…åŒ…	conda instal package_name
    æŸ¥çœ‹å½“å‰ç¯å¢ƒåŒ…åˆ—è¡¨	conda list
    æŸ¥çœ‹æŒ‡å®šç¯å¢ƒåŒ…åˆ—è¡¨	conda list -n environment_name
    æŸ¥çœ‹condaæºä¸­åŒ…çš„ä¿¡æ¯	conda search package_name
    æ›´æ–°åŒ…	conda update package_name
    åˆ é™¤åŒ…	conda remove package_name
    æ¸…ç†æ— ç”¨çš„å®‰è£…åŒ…	conda clean -p
    æ¸…ç†taråŒ…	conda clean -t
    æ¸…ç†æ‰€æœ‰å®‰è£…åŒ…åŠcache	conda clean -y --all
    æ›´æ–°anaconda	conda update annaconda
    æœ€åä¸‰ä¸ªæ¸…ç†å‘½ä»¤ç±»ä¼¼äºæ¸…ç†æ‰‹æœºä¸Šçš„å®‰è£…åŒ…ã€ç¼“å­˜ï¼Œä¸ä¼šåˆ é™¤æŸä¸ªåº“ï¼Œåªæ˜¯åˆ é™¤å·²ç»å®‰è£…å®Œæˆçš„é‚£äº›å®‰è£…åŒ…ã€‚
### æ›´æ¢æº
    1. æ›´æ¢æ¸…åæº
    windows:
    å‘½ä»¤è¡Œä¸­ç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤
    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge 
    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
    è®¾ç½®æœç´¢æ—¶æ˜¾ç¤ºé€šé“åœ°å€
    conda config --set show_channel_urls yes
    linux:
    å°†ä»¥ä¸Šé…ç½®æ–‡ä»¶å†™åœ¨~/.condarcä¸­ vim ~/.condarc
    2. æ›´æ¢ä¸­ç§‘å¤§æº
    conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/
    conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
    conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/
    conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/
    conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/
    conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/ 
    conda config --set show_channel_urls yes
    3. æ˜¾ç¤ºç°æœ‰å®‰è£…æº
    conda config --show channels
    4. æ¢å¤é»˜è®¤æº
    conda config --remove-key channels
    5. ç§»é™¤æŸä¸ªæº
    conda config --remove channels https://mirrors.cloud.tencent.com/anaconda/pkgs/pro/
## csv2xml
    from xml.etree.ElementTree import Element, ElementTree, SubElement
    import csv
    import os


    def csv_to_xml(file_name):
        with open(file_name, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)

            root = Element("sentences")
            d = {
                '1': 'positive',
                '0': 'neutral',
                '-1': 'negative',
                '-2': 'NULL'
            }

            for row in reader:
                # for index, column in enumerate(row):
                node_sentence = Element('sentence')
                root.append(node_sentence)
                node_text = SubElement(node_sentence, 'text')
                # ç»™å¶å­èŠ‚ç‚¹textè®¾ç½®ä¸€ä¸ªæ–‡æœ¬èŠ‚ç‚¹ï¼Œç”¨äºæ˜¾ç¤ºæ–‡æœ¬å†…å®¹
                node_text.text = row[1]
                node_aspectCategories = Element("aspectCategories")
                node_sentence.append(node_aspectCategories)
                node_aspectCategory = Element("aspectCategory", {'category': 'Location#Transportation', 'polarity': d[row[3]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Location#Downtown', 'polarity': d[row[4]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Location#Easy_to_find', 'polarity': d[row[5]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Service#Queue', 'polarity': d[row[6]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Service#Hospitality', 'polarity': d[row[7]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Service#Parking', 'polarity': d[row[8]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Service#Timely', 'polarity': d[row[9]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Price#Level', 'polarity': d[row[10]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Price#Cost_effective', 'polarity': d[row[11]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Price#Discount', 'polarity': d[row[12]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Ambience#Decoration', 'polarity': d[row[13]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Ambience#Noise', 'polarity': d[row[14]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Ambience#Space', 'polarity': d[row[15]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Ambience#Sanitary', 'polarity': d[row[16]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Food#Portion', 'polarity': d[row[17]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Food#Taste', 'polarity': d[row[18]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Food#Appearance', 'polarity': d[row[19]]})
                node_aspectCategories.append(node_aspectCategory)
                node_aspectCategory = Element("aspectCategory", {'category': 'Food#Recommend', 'polarity': d[row[20]]})
                node_aspectCategories.append(node_aspectCategory)
        beatau(root)
        return ElementTree(root)


    def beatau(e, level=0):
        if len(e) > 0:
            e.text = '\n' + '\t' * (level + 1)
            for child in e:
                beatau(child, level + 1)
            child.tail = child.tail[:-1]
        e.tail = '\n' + '  ' * level

        # å¼€å§‹å†™xmlæ–‡æ¡£
        # with open(path, "w", encoding="utf-8") as f:
        #     # writexml()ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ç›®æ ‡æ–‡ä»¶å¯¹è±¡ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯æ ¹èŠ‚ç‚¹çš„ç¼©è¿›æ ¼å¼ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯å…¶ä»–å­èŠ‚ç‚¹çš„ç¼©è¿›æ ¼å¼ï¼Œ
        #     # ç¬¬å››ä¸ªå‚æ•°åˆ¶å®šäº†æ¢è¡Œæ ¼å¼ï¼Œç¬¬äº”ä¸ªå‚æ•°åˆ¶å®šäº†xmlå†…å®¹çš„ç¼–ç ã€‚
        #     doc.writexml(f, indent='', addindent='\t', newl='\n', encoding="utf-8")


    if __name__ == '__main__':
        base_path = os.path.join('xml_file')
        if not os.path.exists(base_path):
            os.mkdir(base_path)

        raw_train_path = os.path.join(base_path, 'raw/train.xml')
        raw_val_path = os.path.join(base_path, 'raw/val.xml')
        raw_test_path = os.path.join(base_path, 'raw/test.xml')

        raw_train_sample_path = os.path.join(base_path, 'raw/train_sample.xml')
        raw_val_sample_path = os.path.join(base_path, 'raw/val_sample.xml')
        raw_test_sample_path = os.path.join(base_path, 'raw/test_sample.xml')


        train = '/home/jye/ASAP/data/train.csv'
        ET = csv_to_xml(train)
        ET.write(raw_train_path, encoding='utf-8')

        val = '/home/jye/ASAP/data/dev.csv'
        ET = csv_to_xml(val)
        ET.write(raw_val_path, encoding='utf-8')

        test = '/home/jye/ASAP/data/test.csv'
        ET = csv_to_xml(test)
        ET.write(raw_test_path, encoding='utf-8')

        train_sample = '/home/jye/ASAP/data/train_sample.csv'
        ET = csv_to_xml(train_sample)
        ET.write(raw_train_sample_path, encoding='utf-8')

        val_sample = '/home/jye/ASAP/data/dev_sample.csv'
        ET = csv_to_xml(val_sample)
        ET.write(raw_val_sample_path, encoding='utf-8')

        test_sample = '/home/jye/ASAP/data/test_sample.csv'
        ET = csv_to_xml(test_sample)
        ET.write(raw_test_sample_path, encoding='utf-8')

# D
## Debug
### Load BERT
    æˆ‘æƒ³loadä¸€ä¸ªè‡ªå·±pretrainçš„bertï¼Œåœ¨loadçš„æ—¶å€™å‡ºç°å‚æ•°çš„å‘½åé—®é¢˜ï¼Œåªè¦ä¸ä¸¥æ ¼loadå°±å¥½äº†ï¼Œå¿½ç•¥é‚£éƒ¨åˆ†å‚æ•°ï¼ˆstrict=Falseï¼‰ 
    https://blog.csdn.net/qq_40420192/article/details/116460475?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.pc_relevant_default&utm_relevant_index=6 
    ç”¨torch loadå‚æ•°çš„æ—¶å€™ï¼Œå› ä¸ºæ–‡ä»¶åæ˜¯stræ ¼å¼ï¼Œç›´æ¥loadæ˜¯loadä¸è¿›æ¥çš„ï¼Œè¦ç”¨torch.load() 
    self.model.load_state_dict(torch.load("/home/disk2/jye/SCAPT/results/Amazon/BERT_0417082222/epoch_7_step_125000.pt"),strict=False)
### TypeError: 'Tensor' object is not callable

# E
# F
## æœåŠ¡å™¨è”ç½‘
    export ALL_PROXY=socks5://10.1.17.175:7890; export http_proxy=http://10.1.17.175:7890; export https_proxy=http://10.1.17.175:7890
# G
# H
# I
# J
# K
## kodcloud

# L 
## linuxå¸¸ç”¨æŒ‡ä»¤
    1. nvidia-smi
    2. nvidia-smi-L             æŸ¥çœ‹æ˜¾å¡ç±»å‹
    3. watch -n 3 nvidia-smi    åˆ·æ–°ï¼ˆåŠ¨æ€ï¼‰æŸ¥çœ‹gpu
    4. gpustat -i               åŒä¸Š
    5. watch --color -n1 gpustat -cpu
    6. ps -ef | grep python     æŸ¥çœ‹å½“å‰è¿›ç¨‹
    7. df -h                    æŸ¥çœ‹ç¡¬ç›˜æƒ…å†µ
    8. du -h --max-depth=1      æŸ¥çœ‹å½“å‰ç›®å½•ï¼Œå“ªä¸ªæ–‡ä»¶å ç”¨æœ€å¤§
    9. du -sh *                 çœ‹å“ªä¸ªç›®å½•å ç”¨ç©ºé—´å¤§
    10. ps -axu | grep jingy | grep python | wc -l  æŸ¥çœ‹çº¿ç¨‹æ•°é‡
    11. cat /proc/cpuinfo | grep processor | wc -l  æŸ¥çœ‹CPUçº¿ç¨‹æ€»æ•°
    12. cat /proc/cpuinfo                           æŸ¥çœ‹CPUä¿¡æ¯
    13. source ~/.bashrc 
    14. netstat -nultp ç»ˆç«¯æŸ¥çœ‹   
    15. df -m                   æŸ¥çœ‹ç³»ç»Ÿç¡¬ç›˜å†…å­˜æƒ…å†µï¼ˆä»¥Mbä¸ºå•ä½ï¼‰
    16. mv -v <æºæ–‡ä»¶åœ°å€> <ç›®çš„æ–‡ä»¶åœ°å€>   ç”¨äºç§»åŠ¨æ–‡ä»¶ä½ç½®
    17. rm -rf ç›®å½•åå­—   ç”¨äºåˆ é™¤æ–‡ä»¶å¤¹  (åˆ é™¤äº†å°±å†æ‰¾ä¸å›æ¥äº†)
## Loss
### CE loss
#### è®¾ç½®ä¸åŒçš„class weight å’Œ sample weight

        batch_size = 10
        nb_classes = 2

        model = nn.Linear(10, nb_classes)
        weight = torch.empty(nb_classes).uniform_(0, 1)
        # åˆå§‹åŒ–CrossEntropyå‡½æ•°æ—¶ä¼ å…¥å„ä¸ªclassçš„æƒé‡, 
        # ä¸”è®¾ç½®reductionä¸ºNoneè¡¨ç¤ºä¸è¿›è¡Œèšåˆï¼Œè¿”å›ä¸€ä¸ªlossæ•°ç»„
        criterion = nn.CrossEntropyLoss(weight=weight, reduction='none')

        # This would be returned from your DataLoader
        x = torch.randn(batch_size, 10)
        target = torch.empty(batch_size, dtype=torch.long).random_(nb_classes)
        sample_weight = torch.empty(batch_size).uniform_(0, 1)

        output = model(x)
        loss = criterion(output, target)
        # å„ä¸ªæ ·æœ¬ä¹˜ä»¥å…¶æƒé‡ï¼Œç„¶åæ±‚å‡å€¼
        loss = loss * sample_weight
        loss.mean().backward()

* [è®¾ç½®ä¸åŒçš„class weight å’Œ sample weight](https://www.jianshu.com/p/8c8169d2204c)
### Triplet Loss
* [ä¸‰å…ƒç»„æŸå¤±ä»‹ç»](https://blog.csdn.net/zenglaoshi/article/details/106928204)
* [ä¸‰å…ƒç»„æŸå¤±è®ºæ–‡](https://arxiv.org/pdf/1503.03832.pdf)
* [ä¸‰å…ƒç»„æŸå¤±ä»£ç 1](https://github.com/AbnerHqC/GaitSet/blob/master/model/network/triplet.py)
* [ä¸‰å…ƒç»„æŸå¤±ä»£ç 2](https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py)
* [triphard å®ç°ä»£ç ](https://blog.csdn.net/qq_32523711/article/details/103826600)
* [ä¸‰å…ƒç»„æŸå¤±ä»£ç è§£è¯»3](https://blog.csdn.net/mos1896/article/details/109401079?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161950900616780366552628%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161950900616780366552628&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-109401079.first_rank_v2_pc_rank_v29&utm_term=%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1)
* [ä¸ä½¿ç”¨hardTripletçš„æƒ…å†µï¼šnä¸ªæ ·æœ¬ï¼Œç®—å‡ºnxnçš„è·ç¦»çŸ©é˜µ](https://github.com/taylover-pei/SSDG-CVPR2020/blob/master/loss/hard_triplet_loss.py)
       
        class TripletLoss(nn.Module):
            """Triplet loss with hard positive/negative mining.
            
            Reference:
                Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.
            
            Imported from `<https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py>`_.
            
            Args:
                margin (float, optional): margin for triplet. Default is 0.3.
            """
            
            def __init__(self, margin=0.3,global_feat, labels):
                super(TripletLoss, self).__init__()
                self.margin = margin
                # https://pytorch.org/docs/1.2.0/nn.html?highlight=marginrankingloss#torch.nn.MarginRankingLoss
                # è®¡ç®—ä¸¤ä¸ªå¼ é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä¸¤å¼ é‡ä¹‹é—´çš„è·ç¦»>marginï¼Œloss ä¸ºæ­£ï¼Œå¦åˆ™loss ä¸º 0
                self.ranking_loss = nn.MarginRankingLoss(margin=margin)
        
            def forward(self, inputs, targets):
                """
                Args:
                    inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).
                    targets (torch.LongTensor): ground truth labels with shape (num_classes).
                """
                n = inputs.size(0)	# batch_size
                
                # Compute pairwise distance, replace by the official when merged
                dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)
                dist = dist + dist.t()
                dist.addmm_(1, -2, inputs, inputs.t())
                dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability
                
                # For each anchor, find the hardest positive and negative
                mask = targets.expand(n, n).eq(targets.expand(n, n).t())
                dist_ap, dist_an = [], []
                for i in range(n):
                    dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))
                    dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))
                dist_ap = torch.cat(dist_ap)
                dist_an = torch.cat(dist_an)
                
                # Compute ranking hinge loss
                y = torch.ones_like(dist_an)
                loss = self.ranking_loss(dist_an, dist_ap, y)
                return loss
# M
# N
# O
# P
## Pycharmæœç´¢ï¼ˆæ–‡ä»¶ã€å‡½æ•°ã€å†…å®¹ï¼‰
    1. æ–‡ä»¶å†…æ£€ç´¢Ctrl + F
    2. æ–‡ä»¶å†…æ›¿æ¢Ctrl + R
    3. é¡¹ç›®ä¸­æŸ¥æ‰¾
    Ctrl + Shift + F
    è¯¥å¿«æ·é”®å®¹æ˜“å†²çªï¼Œæ¯”å¦‚æœ¬åœ°å¦‚æœå®‰è£…äº†æœç‹—è¾“å…¥æ³•ï¼Œå¯ä»¥å…ˆå°†å¯¹åº”çš„å¿«æ·é”®å…³é—­å†ä½¿ç”¨ã€‚
    Shift + Shift
    å¿«æ·é”®åŒå‡»Shiftï¼Œå¯ä»¥æ›´ç²¾ç¡®çš„æŸ¥æ‰¾åˆ°ç±»å/å‡½æ•°å/æ–‡ä»¶å
    å‹¾é€‰Include non-project itemsï¼Œå¯ä»¥æœç´¢é¡¹ç›®ä»£ç ä¹‹å¤–çš„å†…å®¹ï¼Œæ¯”å¦‚å¼•å…¥çš„åº“
    Classesæœç´¢å¹¶è·³è½¬ç‰¹å®šçš„ç±»ï¼Œå¿«æ·é”®Ctrl + Nã€‚
    Fileså¯ä»¥å¿«é€Ÿè·³è½¬åˆ°æ–‡ä»¶ï¼Œæ¯”å¦‚æˆ‘è¾“å…¥cï¼Œå°±ä¼šæ£€ç´¢å‡ºæ‰€æœ‰ä¸Cç›¸å…³çš„æ–‡ä»¶ï¼Œå¿«æ·é”®Ctrl + Shift + N
    å¦å¤–Symbolsçš„æ¨¡ç³ŠæŸ¥è¯¢ä¹Ÿéå¸¸å®ç”¨ã€‚å½“è®°ä¸æ¸…å®Œæ•´çš„å…³é”®è¯æ—¶ï¼Œå¯ä»¥è¿›è¡Œæ¨¡ç³Šæœç´¢ã€‚å¿«æ·é”®Ctrl + Alt + Shift + N
    4. å½“å‰ç±»ã€æ–¹æ³•ã€å±æ€§åˆ—è¡¨
    å¿«æ·é”®Ctrl + F12ï¼Œ å¯ä»¥æŠŠå½“å‰æ–‡ä»¶ä¸­çš„æ‰€æœ‰å±æ€§ã€ç±»ã€æ–¹æ³•éƒ½æ˜¾ç¤ºå‡ºæ¥
    ç›´æ¥è¾“å…¥å…³é”®å­—ï¼Œå°±å¯ä»¥æ£€ç´¢å‡ºç¬¦åˆæ¡ä»¶çš„å±æ€§/ç±»/æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å®šä½åˆ°ç›¸å…³ä½ç½®
    åŒæ ·çš„åŠŸèƒ½ï¼Œä¹Ÿå¯ä»¥é€šè¿‡Alt + 7 æ¥å®ç°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚åŒæ ·ä¹Ÿæ˜¯ç›´æ¥ç›´æ¥è¾“å…¥å…³é”®å­—è¿›è¡Œæœç´¢ã€‚
    5. æŸ¥çœ‹æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
    å¿«æ·é”®ctrl + eã€‚å¯ä»¥æŸ¥çœ‹æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
    6. æŸ¥çœ‹å‡½æ•°çš„è°ƒç”¨å…³ç³»
    å½“ä¸€ä¸ªå‡½æ•°ä¸çŸ¥é“è¢«å“ªäº›åœ°æ–¹è°ƒç”¨çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡å¿«æ·é”®Alt + F7 è¿›è¡ŒæŸ¥çœ‹
## Pytorch load checkpointså‡ºé”™
    RuntimeError: Attempting to deserialize object on CUDA device 4 but torch.cuda.device_count() is 4. Please use torch.load with map_location to map your storages to an existing device.
    è§£å†³ï¼šmodel = torch.load(model_path, map_location='cuda:0')
# Q
# R
## è½¯ä»¶å­¦æŠ¥å‚è€ƒæ–‡çŒ®æ ¼å¼zetora
[è½¯ä»¶å­¦æŠ¥å‚è€ƒæ–‡çŒ®æ ¼å¼zetora](https://blog.csdn.net/still_night/article/details/104838490)
# S
# T
## transformersåº“
    tokenizer2 = RobertaTokenizer.from_pretrained("roberta-base") 
    inputs=tokenizer2('great food but the service was dreadful !', 'food',return_tensors="pt") 
    print(inputs) 
    {'input_ids': tensor([[    0, 12338,   689,    53,     5,   544,    21, 31715, 27785,     2,    2, 13193,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} 
    tokenizer1 = BertTokenizer.from_pretrained("bert-base-uncased") 
    inputs=tokenizer1('great food but the service was dreadful !', 'food' return_tensors="pt") 
    {'input_ids': tensor([[  101,  2307,  2833,  2021,  1996,  2326,  2001, 21794,   102,  2833,
           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} 
           
å½“è¾“å…¥ä¸¤ä¸ªå¥å­çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨ç¼–ç segment
### å…³äºä½¿ç”¨Bertå’ŒRobertaçš„å¥‡å¥‡æ€ªæ€ªçš„ç»“æœ
    from transformers import RobertaTokenizer, RobertaModel
    from transformers import BertTokenizer, BertModel
    import torch
    tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
    tokenizer1 = BertTokenizer.from_pretrained("bert-base-uncased")
    roberta = RobertaModel.from_pretrained("roberta-base")
    bert = BertModel.from_pretrained('bert-base-uncased')
    inputs1=tokenizer('the staff should be a bit more friendly.','staff',return_tensors="pt")
    inputs2=tokenizer1('the staff should be a bit more friendly.','staff',return_tensors="pt")
    print(inputs1,'\n',inputs2)
    {'input_ids': tensor([[    0,   627,   813,   197,    28,    10,   828,    55,  5192,     4,
                2,     2, 17360,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} 
    {'input_ids': tensor([[ 101, 1996, 3095, 2323, 2022, 1037, 2978, 2062, 5379, 1012,  102, 3095,
            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
    outputs1 = roberta(input_ids=inputs1['input_ids'])
    print(type(outputs1))
    <class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>
    outputs1,_ = roberta(input_ids=inputs1['input_ids'])
    print(type(outputs1))
    <class 'str'>
    outputs2= bert(input_ids=inputs2['input_ids'])
    print(type(outputs2))
    <class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>
    outputs2,_= bert(input_ids=inputs2['input_ids'])
    print(type(outputs2))
    <class 'str'>
    outputs2,_= bert(input_ids=inputs2['input_ids'],token_type_ids=inputs2['token_type_ids'])
    print(type(outputs2))
    <class 'str'>

## tmux
    tmux                        å¯åŠ¨tmux
    exit æˆ– Ctrl+D              é€€å‡ºtmux
    tmux ls                     å½“å‰æ‰€æœ‰çš„tmuxä¼ªçª—å£
    tmux attach -t 0            é‡æ¥ä¼šè¯ï¼Œä½¿ç”¨ä¼ªçª—å£ç¼–å·
    tmux attach -t <name>       é‡æ¥ä¼šè¯ ä½¿ç”¨ä¼ªçª—å£åç§°
    tmux kill-session -t 0      æ€æ­»ä¼šè¯
    tmux kill-session -t <name> æ€æ­»ä¼šè¯
    tmux switch -t 0            åˆ‡æ¢ä¼šè¯
    tmux switch -t <session-name>
    tmux rename-session -t 0 <new-name>  é‡å‘½åä¼šè¯
    tmux list-keys              åˆ—å‡ºæ‰€æœ‰å¿«æ·é”®ï¼ŒåŠå…¶å¯¹åº”çš„ Tmux å‘½ä»¤
    ä¼šè¯å¿«æ·é”®
    Ctrl+b dï¼š                  åˆ†ç¦»å½“å‰ä¼šè¯ã€‚
    Ctrl+b sï¼š                  åˆ—å‡ºæ‰€æœ‰ä¼šè¯ã€‚
    Ctrl+b $ï¼š                  é‡å‘½åå½“å‰ä¼šè¯ã€‚
    çª—æ ¼å¿«æ·é”®
    Ctrl+b %ï¼š                  åˆ’åˆ†å·¦å³ä¸¤ä¸ªçª—æ ¼ã€‚
    Ctrl+b "ï¼š                  åˆ’åˆ†ä¸Šä¸‹ä¸¤ä¸ªçª—æ ¼ã€‚
    Ctrl+b <arrow key>ï¼š        å…‰æ ‡åˆ‡æ¢åˆ°å…¶ä»–çª—æ ¼ã€‚æ˜¯æŒ‡å‘è¦åˆ‡æ¢åˆ°çš„çª—æ ¼çš„æ–¹å‘é”®ï¼Œæ¯”å¦‚åˆ‡æ¢åˆ°ä¸‹æ–¹çª—æ ¼ï¼Œå°±æŒ‰æ–¹å‘é”®â†“ã€‚
    Ctrl+b ;ï¼š                  å…‰æ ‡åˆ‡æ¢åˆ°ä¸Šä¸€ä¸ªçª—æ ¼ã€‚
    Ctrl+b oï¼š                  å…‰æ ‡åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªçª—æ ¼ã€‚
    Ctrl+b {ï¼š                  å½“å‰çª—æ ¼å·¦ç§»ã€‚
    Ctrl+b }ï¼š                  å½“å‰çª—æ ¼å³ç§»ã€‚
    Ctrl+b Ctrl+oï¼š             å½“å‰çª—æ ¼ä¸Šç§»ã€‚
    Ctrl+b Alt+oï¼š              å½“å‰çª—æ ¼ä¸‹ç§»ã€‚
    Ctrl+b xï¼š                  å…³é—­å½“å‰çª—æ ¼ã€‚
    Ctrl+b !ï¼š                  å°†å½“å‰çª—æ ¼æ‹†åˆ†ä¸ºä¸€ä¸ªç‹¬ç«‹çª—å£ã€‚
    Ctrl+b zï¼š                  å½“å‰çª—æ ¼å…¨å±æ˜¾ç¤ºï¼Œå†ä½¿ç”¨ä¸€æ¬¡ä¼šå˜å›åŸæ¥å¤§å°ã€‚
    Ctrl+b Ctrl+<arrow key>ï¼š   æŒ‰ç®­å¤´æ–¹å‘è°ƒæ•´çª—æ ¼å¤§å°ã€‚
    Ctrl+b qï¼š                  æ˜¾ç¤ºçª—æ ¼ç¼–å·
## tensorboardx
* å®‰è£…ï¼špip install tensorboardX
* [ä½¿ç”¨æ–¹æ³•å‚è€ƒ](https://blog.csdn.net/bigbennyguo/article/details/87956434)
* [è®¿é—®è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„tensorboardæ–¹æ³•ï¼ˆæˆåŠŸï¼‰](https://www.cnblogs.com/monologuesmw/p/14465117.html)
* [è®¿é—®è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„tensorboardæ–¹æ³•ï¼ˆæˆåŠŸï¼‰](https://blog.csdn.net/power_kaikaige/article/details/125111527?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0-125111527-blog-104371497.pc_relevant_aa&spm=1001.2101.3001.4242.1&utm_relevant_index=3)
* [ç¨³å®šæˆåŠŸ](https://blog.csdn.net/weixin_35653315/article/details/71327740)

        1 . è¿æ¥sshæ—¶ï¼Œå°†æœåŠ¡å™¨çš„6006ç«¯å£é‡å®šå‘åˆ°è‡ªå·±æœºå™¨ä¸Šæ¥ï¼š

        ssh -L 16006:127.0.0.1:6006 jye@103.242.173.103
        ssh -L 16006:127.0.0.1:6006 jye@172.18.31.58
        ssh -L 16006:127.0.0.1:6006 jye@172.18.31.54

        å…¶ä¸­ï¼š16006:127.0.0.1ä»£è¡¨è‡ªå·±æœºå™¨ä¸Šçš„16006å·ç«¯å£ï¼Œ6006æ˜¯æœåŠ¡å™¨ä¸Štensorboardä½¿ç”¨çš„ç«¯å£ã€‚

        2 . åœ¨æœåŠ¡å™¨ä¸Šä½¿ç”¨6006ç«¯å£æ­£å¸¸å¯åŠ¨tensorboardï¼š

        tensorboard --logdir=runs --port=6006

        3 . åœ¨æœ¬åœ°æµè§ˆå™¨ä¸­è¾“å…¥åœ°å€ï¼š
        
        127.0.0.1:16006

xshelléš§é“+åœ¨æœåŠ¡å™¨ä¸Šè·³è½¬è‡³ç›¸åº”çš„ç›®å½•ä¸‹ï¼Œè¾“å…¥å‘½ä»¤tensorboard --logdir=<æ–‡ä»¶åç§°>  

## ç”»å›¾
### t-SNE 
* [t-sne](https://zhuanlan.zhihu.com/p/358195652)

        import numpy as np
        import matplotlib.pyplot as plt
        import json
        from sklearn import manifold

        def draw_tsne(emb_filename):
            model = json.load(open(emb_filename, 'r'))
            X = np.array(model['features'])
            y = np.array(model['label'])
            '''t-SNE'''
            tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)
            X_tsne = tsne.fit_transform(X)
            print(X.shape)
            print(X_tsne.shape)
            print(y.shape)
            print("Org data dimension is {}.Embedded data dimension is {}".format(X.shape[-1], X_tsne.shape[-1]))

            '''visualize'''
            x_min, x_max = X_tsne.min(0), X_tsne.max(0)
            X_norm = (X_tsne - x_min) / (x_max - x_min)  # å½’ä¸€åŒ–

            plt.figure(figsize=(8, 8))
            for i in range(X_norm.shape[0]):
                plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i]), color=plt.cm.Set1(y[i]),
                        fontdict={'weight': 'bold', 'size': 10})
            plt.xticks([])
            plt.yticks([])
            plt.suptitle('Roberta-SPC CE', fontsize=24)
            plt.show()


        emb_filename = ("/home/disk2/jye/ABSA_Curriculum_Learning/roberta-absa/roberta_spc_total_test1.json")
        draw_tsne(emb_filename)

### è®­ç»ƒè¿‡ç¨‹å›¾
    import matplotlib
    import matplotlib.pyplot as plt
    matplotlib.use('agg')

    def plot_jasons_lineplot(x, y, x_label, y_label, title, output_png_path):
        if x == None:
            x = range(1, len(y) + 1)

        _, ax = plt.subplots()
        plt.plot(x, y, linewidth=1)

        plt.xlabel(x_label)
        plt.ylabel(y_label)
        plt.title(title)

        plt.tight_layout()
        plt.savefig(output_png_path, dpi=400)
        plt.clf()

        print(f"plot saved at {output_png_path}")
# W
## wordå°æŠ€èƒ½ï¼š
 * ctrl+H æ‰¹é‡æ›¿æ¢
# Y
## äº¿å›¾å›¾ç¤º
[ä¸‹è½½åœ°å€](pan.baidu.com/s/1iW9Jy9ljM04angzzkoyzQQ?pwd=1234 æå–ç : 1234)
#  *#*
## æ¢¯å­
    https://jinkela.bar/auth/register?code=WPP2
    https://foosber.com/auth/login
## æ¢¯å­autoè„šæœ¬æ”¹å†™
    -
        name: auto_group
        type: url-test
        proxies:
        - 'Aperture ç¾å›½ | IPv6 ç›´è¿ | NATIVE | V2'
        - 'Hinet å°æ¹¾ | IPv6 CDN | å®½é¢‘ | V1'
        - 'Metfone æŸ¬åŸ”å¯¨ | IPv6 ç›´è¿ | 0.5x | å®½é¢‘ | V1'
        - 'OCI æ–°åŠ å¡ | IPv6 ç›´è¿ | NATIVE | V1'
        - 'PCCW é¦™æ¸¯ | IPv6 ç›´è¿ | å®½é¢‘ | V2'
        - 'Softbank æ—¥æœ¬ | IPv6 ç›´è¿ | å®½é¢‘ | V2'
        url: 'http://www.gstatic.com/generate_204'
        interval: '600'
    -
    
        name: ğŸ”°å›½å¤–æµé‡
        type: select
        proxies:
        - 'auto_group'

## åå°å¹¶è¡Œï¼ˆå¹¶è¡Œäº†ä½†è¿˜æ²¡æœ‰å®Œå…¨å¹¶è¡Œï¼‰
        python -u CL_111.py --model_name bert_spc --curriculum_mode change_sample_weight --dataset restaurant --seed 8 --device cuda:9  &
        python -u CL.py --model_name bert_spc --curriculum_mode two_stage --dataset restaurant --seed 8 --device cuda:8  &
        python -u CL.py --model_name bert_spc --curriculum_mode gradual --dataset restaurant --seed 8 --device cuda:7  &
        python -u train.py --model_name bert_spc --train_mode CE+TL --dataset restaurant --seed 8 --device cuda:6   
        sleep  20m  

        python -u CL_111.py --seed 42 --dataset restaurant --device cuda:9  &
        python -u CL_111.py --seed 41 --dataset restaurant --device cuda:8  &
        python -u CL_111.py --seed 40 --dataset restaurant --device cuda:7 

        å‰å°ç¨‹åºè·‘å®Œäº†åæ‰ä¼šæ‰§è¡Œsleepï¼ˆç›®çš„æ˜¯ç­‰åå°çš„æ‰€æœ‰ç¨‹åºéƒ½è·‘å®Œï¼‰ï¼Œç„¶åå›é¡ºåºæ‰§è¡Œä¸‹æ–¹çš„ç¨‹åº 